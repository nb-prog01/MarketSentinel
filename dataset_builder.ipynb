{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f0904d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas version: 2.1.4\n",
      "\n",
      "Checking source file at: final_output/feature_store/final_features_fixed_f.parquet\n",
      "‚úÖ Source parquet file exists and is readable.\n",
      "Rows loaded (sanity only): 257860\n",
      "\n",
      "Cell 1 PASSED ‚Äî Environment and source file validated.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment & Source File Validation\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "\n",
    "# ---- Contract: Single Source of Truth ----\n",
    "PARQUET_PATH = \"final_output/feature_store/final_features_fixed_f.parquet\"\n",
    "\n",
    "print(f\"\\nChecking source file at: {PARQUET_PATH}\")\n",
    "\n",
    "if not os.path.exists(PARQUET_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Contract violation: source file not found at {PARQUET_PATH}\"\n",
    "    )\n",
    "\n",
    "# Try opening parquet (no inspection)\n",
    "try:\n",
    "    _df = pd.read_parquet(PARQUET_PATH)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"‚ùå Contract violation: failed to read parquet file.\\nError: {e}\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Source parquet file exists and is readable.\")\n",
    "print(\"Rows loaded (sanity only):\", len(_df))\n",
    "\n",
    "# IMPORTANT: Do not inspect columns yet\n",
    "del _df\n",
    "\n",
    "print(\"\\nCell 1 PASSED ‚Äî Environment and source file validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fd217c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded.\n",
      "Shape: (257860, 268)\n",
      "‚úÖ Mandatory metadata columns present: ['date', 'symbol']\n",
      "‚úÖ Mandatory target columns present: ['ret_fwd_1d', 'ret_fwd_5d']\n",
      "\n",
      "Cell 2 PASSED ‚Äî Required columns validated.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Dataset & Mandatory Column Validation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PARQUET_PATH = \"final_output/feature_store/final_features_fixed_f.parquet\"\n",
    "\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "print(\"Dataset loaded.\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# ---- Contract: Mandatory Columns ----\n",
    "REQUIRED_METADATA_COLS = {\"symbol\", \"date\"}\n",
    "REQUIRED_TARGET_COLS = {\"ret_fwd_1d\", \"ret_fwd_5d\"}\n",
    "\n",
    "missing_meta = REQUIRED_METADATA_COLS - set(df.columns)\n",
    "missing_targets = REQUIRED_TARGET_COLS - set(df.columns)\n",
    "\n",
    "if missing_meta:\n",
    "    raise ValueError(\n",
    "        f\"‚ùå Contract violation: Missing metadata columns: {sorted(missing_meta)}\"\n",
    "    )\n",
    "\n",
    "if missing_targets:\n",
    "    raise ValueError(\n",
    "        f\"‚ùå Contract violation: Missing target columns: {sorted(missing_targets)}\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Mandatory metadata columns present:\", sorted(REQUIRED_METADATA_COLS))\n",
    "print(\"‚úÖ Mandatory target columns present:\", sorted(REQUIRED_TARGET_COLS))\n",
    "\n",
    "print(\"\\nCell 2 PASSED ‚Äî Required columns validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a6f440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No NaNs in metadata columns\n",
      "‚úÖ 'date' column is parseable as datetime\n",
      "\n",
      "Cell 3 PASSED ‚Äî Metadata integrity confirmed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Metadata Integrity Validation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Metadata NaN check ----\n",
    "if df[\"symbol\"].isna().any():\n",
    "    raise ValueError(\"‚ùå Contract violation: NaNs found in 'symbol' column\")\n",
    "\n",
    "if df[\"date\"].isna().any():\n",
    "    raise ValueError(\"‚ùå Contract violation: NaNs found in 'date' column\")\n",
    "\n",
    "print(\"‚úÖ No NaNs in metadata columns\")\n",
    "\n",
    "# ---- Date parsing (no mutation yet) ----\n",
    "try:\n",
    "    parsed_dates = pd.to_datetime(df[\"date\"], errors=\"raise\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"‚ùå Contract violation: 'date' column is not parseable as datetime\\n{e}\")\n",
    "\n",
    "print(\"‚úÖ 'date' column is parseable as datetime\")\n",
    "\n",
    "print(\"\\nCell 3 PASSED ‚Äî Metadata integrity confirmed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8df8fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Symbols canonicalized successfully\n",
      "\n",
      "Sample canonical symbols:\n",
      "['AAPL', 'ABBV', 'AMZN', 'ASML', 'BA', 'BABA', 'BAC', 'BHP', 'BP', 'BTC_USD']\n",
      "\n",
      "Cell 4 PASSED ‚Äî Symbol namespace locked.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Symbol Canonicalization\n",
    "\n",
    "import re\n",
    "\n",
    "def canonicalize_symbol(sym: str) -> str:\n",
    "    \"\"\"\n",
    "    Canonical symbol format:\n",
    "    - Uppercase\n",
    "    - Replace non-alphanumeric characters with underscore\n",
    "    - Collapse multiple underscores\n",
    "    - Strip leading/trailing underscores\n",
    "    \"\"\"\n",
    "    sym = str(sym).upper()\n",
    "    sym = re.sub(r\"[^A-Z0-9]\", \"_\", sym)\n",
    "    sym = re.sub(r\"_+\", \"_\", sym)\n",
    "    sym = sym.strip(\"_\")\n",
    "    return sym\n",
    "\n",
    "# Apply canonicalization\n",
    "df[\"symbol\"] = df[\"symbol\"].apply(canonicalize_symbol)\n",
    "\n",
    "# ---- Validation ----\n",
    "if df[\"symbol\"].isna().any():\n",
    "    raise ValueError(\"‚ùå Contract violation: NaNs introduced during symbol canonicalization\")\n",
    "\n",
    "if (df[\"symbol\"].str.len() == 0).any():\n",
    "    raise ValueError(\"‚ùå Contract violation: Empty symbol after canonicalization\")\n",
    "\n",
    "if not df[\"symbol\"].map(lambda x: isinstance(x, str)).all():\n",
    "    raise ValueError(\"‚ùå Contract violation: Non-string symbol detected after canonicalization\")\n",
    "\n",
    "print(\"‚úÖ Symbols canonicalized successfully\")\n",
    "\n",
    "# Optional sanity peek (safe)\n",
    "print(\"\\nSample canonical symbols:\")\n",
    "print(df[\"symbol\"].drop_duplicates().head(10).tolist())\n",
    "\n",
    "print(\"\\nCell 4 PASSED ‚Äî Symbol namespace locked.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1778529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature schema frozen with 264 features\n",
      "üìÑ Written to: D:\\MarketSentinel\\MarketSentinel\\feature_schema.txt\n",
      "\n",
      "First 10 feature columns:\n",
      " - atr_14\n",
      " - atr_14_rmean_60\n",
      " - atr_14_rstd_60\n",
      " - atr_14_rz_60\n",
      " - close_z_20\n",
      " - close_z_20_rmean_60\n",
      " - close_z_20_rstd_60\n",
      " - close_z_20_rz_60\n",
      " - cpi_change\n",
      " - cpi_yoy\n",
      "\n",
      "Cell 5 PASSED ‚Äî Feature schema frozen.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Feature Discovery & Schema Freeze\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Contract definitions ----\n",
    "METADATA_COLS = {\"symbol\", \"date\"}\n",
    "TARGET_COLS = {\"ret_fwd_1d\", \"ret_fwd_5d\"}\n",
    "\n",
    "# Discover feature columns\n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in METADATA_COLS and col not in TARGET_COLS\n",
    "]\n",
    "\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError(\"‚ùå Contract violation: No feature columns discovered\")\n",
    "\n",
    "# Deterministic ordering\n",
    "feature_cols = sorted(feature_cols)\n",
    "\n",
    "# Safety checks\n",
    "forbidden = METADATA_COLS.union(TARGET_COLS)\n",
    "leaked = forbidden.intersection(feature_cols)\n",
    "\n",
    "if leaked:\n",
    "    raise ValueError(\n",
    "        f\"‚ùå Contract violation: Forbidden columns leaked into feature set: {sorted(leaked)}\"\n",
    "    )\n",
    "\n",
    "# ---- Freeze schema ----\n",
    "schema_path = Path(\"feature_schema.txt\")\n",
    "\n",
    "with open(schema_path, \"w\") as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(col + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Feature schema frozen with {len(feature_cols)} features\")\n",
    "print(f\"üìÑ Written to: {schema_path.resolve()}\")\n",
    "\n",
    "# Safe summary\n",
    "print(\"\\nFirst 10 feature columns:\")\n",
    "for c in feature_cols[:10]:\n",
    "    print(\" -\", c)\n",
    "\n",
    "print(\"\\nCell 5 PASSED ‚Äî Feature schema frozen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "234ca239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped features: ['is_month_end']\n",
      "üîí Feature schema RE-FROZEN with 263 features\n",
      "üìÑ Written to: D:\\MarketSentinel\\MarketSentinel\\feature_schema.txt\n",
      "\n",
      "Cell 5B PASSED ‚Äî Schema re-frozen after drop.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5B: Drop Non-Numeric Features & Re-freeze Schema\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Explicitly dropped features ----\n",
    "DROPPED_FEATURES = {\"is_month_end\"}\n",
    "\n",
    "missing = DROPPED_FEATURES - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"‚ùå Attempting to drop missing columns: {missing}\")\n",
    "\n",
    "df = df.drop(columns=list(DROPPED_FEATURES))\n",
    "print(f\"‚úÖ Dropped features: {sorted(DROPPED_FEATURES)}\")\n",
    "\n",
    "# ---- Re-discover feature columns ----\n",
    "METADATA_COLS = {\"symbol\", \"date\"}\n",
    "TARGET_COLS = {\"ret_fwd_1d\", \"ret_fwd_5d\"}\n",
    "\n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in METADATA_COLS and col not in TARGET_COLS\n",
    "]\n",
    "\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError(\"‚ùå Contract violation: No feature columns remain after drop\")\n",
    "\n",
    "feature_cols = sorted(feature_cols)\n",
    "\n",
    "# Safety check\n",
    "forbidden = METADATA_COLS.union(TARGET_COLS)\n",
    "leaked = forbidden.intersection(feature_cols)\n",
    "if leaked:\n",
    "    raise ValueError(\n",
    "        f\"‚ùå Contract violation: Forbidden columns leaked into feature set: {leaked}\"\n",
    "    )\n",
    "\n",
    "# ---- Re-freeze schema ----\n",
    "schema_path = Path(\"feature_schema.txt\")\n",
    "with open(schema_path, \"w\") as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(col + \"\\n\")\n",
    "\n",
    "print(f\"üîí Feature schema RE-FROZEN with {len(feature_cols)} features\")\n",
    "print(f\"üìÑ Written to: {schema_path.resolve()}\")\n",
    "\n",
    "print(\"\\nCell 5B PASSED ‚Äî Schema re-frozen after drop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22a0e5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped due to NaN targets: 0\n",
      "‚úÖ All feature columns are numeric\n",
      "‚ö†Ô∏è Values exceed SAFE_MAX before clipping: max=3961173835776.0000\n",
      "‚úÖ Feature sanitation complete\n",
      "‚úÖ All features finite, float32, and clipped\n",
      "\n",
      "Cell 6 PASSED ‚Äî Numerical safety enforced.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 (Revised): Feature Sanitation & Numerical Safety\n",
    "# Adds HARD numeric feature validation\n",
    "\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "SAFE_MAX = 10.0\n",
    "\n",
    "# ---- Drop rows with NaN targets ----\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=[\"ret_fwd_1d\", \"ret_fwd_5d\"])\n",
    "dropped = initial_rows - len(df)\n",
    "\n",
    "print(f\"Rows dropped due to NaN targets: {dropped}\")\n",
    "\n",
    "# ---- Numeric feature validation ----\n",
    "non_numeric_features = [\n",
    "    col for col in feature_cols\n",
    "    if not is_numeric_dtype(df[col])\n",
    "]\n",
    "\n",
    "if non_numeric_features:\n",
    "    raise TypeError(\n",
    "        \"‚ùå Contract violation: Non-numeric feature columns detected.\\n\"\n",
    "        \"These must be removed or engineered explicitly BEFORE this stage:\\n\"\n",
    "        + \"\\n\".join(f\" - {c} ({df[c].dtype})\" for c in non_numeric_features)\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ All feature columns are numeric\")\n",
    "\n",
    "# ---- Sanitize feature columns ----\n",
    "features = df[feature_cols]\n",
    "\n",
    "# Replace inf/-inf with NaN, then NaN ‚Üí 0.0\n",
    "features = features.replace([np.inf, -np.inf], np.nan)\n",
    "features = features.fillna(0.0)\n",
    "\n",
    "# Convert dtype\n",
    "features = features.astype(np.float32)\n",
    "\n",
    "# Detect pre-clip violations\n",
    "pre_clip_max = features.abs().max().max()\n",
    "if pre_clip_max > SAFE_MAX:\n",
    "    print(f\"‚ö†Ô∏è Values exceed SAFE_MAX before clipping: max={pre_clip_max:.4f}\")\n",
    "\n",
    "# Clip to SAFE_MAX\n",
    "features = features.clip(-SAFE_MAX, SAFE_MAX)\n",
    "\n",
    "# Final invariant checks\n",
    "if not np.isfinite(features.values).all():\n",
    "    raise ValueError(\"‚ùå Contract violation: Non-finite values remain in features\")\n",
    "\n",
    "if features.dtypes.nunique() != 1 or features.dtypes.iloc[0] != np.float32:\n",
    "    raise ValueError(\"‚ùå Contract violation: Feature dtype is not float32\")\n",
    "\n",
    "# Write back sanitized features\n",
    "df.loc[:, feature_cols] = features\n",
    "\n",
    "print(\"‚úÖ Feature sanitation complete\")\n",
    "print(\"‚úÖ All features finite, float32, and clipped\")\n",
    "\n",
    "print(\"\\nCell 6 PASSED ‚Äî Numerical safety enforced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae5dd816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Symbols with valid temporal ordering: 66\n",
      "Total possible windows (pre-split): 253900\n",
      "\n",
      "Sample symbol window counts:\n",
      " - AAPL: 3899\n",
      " - ABBV: 3899\n",
      " - AMZN: 3899\n",
      " - ASML: 3899\n",
      " - BA: 3882\n",
      "\n",
      "Cell 7 PASSED ‚Äî Temporal ordering validated.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Temporal Ordering & Window Viability Check\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "\n",
    "# Ensure datetime (safe conversion, already validated earlier)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "viable_symbols = {}\n",
    "invalid_symbols = []\n",
    "\n",
    "for symbol, sdf in df.groupby(\"symbol\"):\n",
    "    sdf = sdf.sort_values(\"date\")\n",
    "\n",
    "    # Check monotonicity\n",
    "    if not sdf[\"date\"].is_monotonic_increasing:\n",
    "        invalid_symbols.append(symbol)\n",
    "        continue\n",
    "\n",
    "    n_rows = len(sdf)\n",
    "    n_windows = n_rows - WINDOW_SIZE + 1\n",
    "\n",
    "    if n_windows > 0:\n",
    "        viable_symbols[symbol] = n_windows\n",
    "\n",
    "if invalid_symbols:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Contract violation: Non-monotonic dates for symbols:\\n\"\n",
    "        + \"\\n\".join(invalid_symbols)\n",
    "    )\n",
    "\n",
    "if not viable_symbols:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Contract violation: No symbols can form a valid 60-step window\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Symbols with valid temporal ordering: {len(viable_symbols)}\")\n",
    "\n",
    "# Summary stats\n",
    "total_windows = sum(viable_symbols.values())\n",
    "print(f\"Total possible windows (pre-split): {total_windows}\")\n",
    "\n",
    "# Safe peek\n",
    "sample = list(viable_symbols.items())[:5]\n",
    "print(\"\\nSample symbol window counts:\")\n",
    "for sym, cnt in sample:\n",
    "    print(f\" - {sym}: {cnt}\")\n",
    "\n",
    "print(\"\\nCell 7 PASSED ‚Äî Temporal ordering validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba0dd759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Window construction complete\n",
      "X shape: (253900, 60, 263)\n",
      "y shape: (253900, 2)\n",
      "Feature dim (F): 263\n",
      "\n",
      "Cell 8 PASSED ‚Äî Temporal windows built.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Temporal Window Construction\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "\n",
    "X_windows = []\n",
    "y_targets = []\n",
    "meta = []\n",
    "\n",
    "for symbol, sdf in df.groupby(\"symbol\"):\n",
    "    sdf = sdf.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    feature_matrix = sdf[feature_cols].values\n",
    "    targets = sdf[[\"ret_fwd_1d\", \"ret_fwd_5d\"]].values\n",
    "    dates = sdf[\"date\"].values\n",
    "\n",
    "    for i in range(len(sdf) - WINDOW_SIZE + 1):\n",
    "        window_X = feature_matrix[i : i + WINDOW_SIZE]\n",
    "        window_y = targets[i + WINDOW_SIZE - 1]\n",
    "        window_date = dates[i + WINDOW_SIZE - 1]\n",
    "\n",
    "        # Hard invariants\n",
    "        if not np.isfinite(window_X).all():\n",
    "            continue\n",
    "        if not np.isfinite(window_y).all():\n",
    "            continue\n",
    "\n",
    "        X_windows.append(window_X)\n",
    "        y_targets.append(window_y)\n",
    "        meta.append((symbol, window_date))\n",
    "\n",
    "X = np.stack(X_windows)\n",
    "y = np.stack(y_targets)\n",
    "\n",
    "print(\"‚úÖ Window construction complete\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Feature dim (F): {X.shape[-1]}\")\n",
    "\n",
    "# Final sanity\n",
    "assert X.shape[0] == y.shape[0] == len(meta)\n",
    "assert X.shape[1] == WINDOW_SIZE\n",
    "assert X.shape[2] == len(feature_cols)\n",
    "\n",
    "print(\"\\nCell 8 PASSED ‚Äî Temporal windows built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77c83f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "Train: 177703\n",
      "Val:   38048\n",
      "Test:  38149\n",
      "  wrote rows 0 ‚Üí 2048\n",
      "  wrote rows 176128 ‚Üí 177703\n",
      "  wrote rows 0 ‚Üí 2048\n",
      "  wrote rows 36864 ‚Üí 38048\n",
      "  wrote rows 0 ‚Üí 2048\n",
      "  wrote rows 36864 ‚Üí 38149\n",
      "‚úÖ HDF5 files written successfully (datetime stored as int64 ns)\n",
      "\n",
      "Cell 9 PASSED ‚Äî Memory-safe persistence complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 (Final): Chunked Chronological Split & HDF5 Write\n",
    "# Fix: store date as int64 nanoseconds\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# ---- Organize windows by symbol ----\n",
    "by_symbol = defaultdict(list)\n",
    "for idx, (symbol, date) in enumerate(meta):\n",
    "    by_symbol[symbol].append(idx)\n",
    "\n",
    "train_idx, val_idx, test_idx = [], [], []\n",
    "\n",
    "for symbol, indices in by_symbol.items():\n",
    "    n = len(indices)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val = int(n * VAL_RATIO)\n",
    "\n",
    "    train_idx.extend(indices[:n_train])\n",
    "    val_idx.extend(indices[n_train:n_train + n_val])\n",
    "    test_idx.extend(indices[n_train + n_val:])\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(\"Train:\", len(train_idx))\n",
    "print(\"Val:  \", len(val_idx))\n",
    "print(\"Test: \", len(test_idx))\n",
    "\n",
    "\n",
    "def write_h5_chunked(path, indices):\n",
    "    n = len(indices)\n",
    "    F = X.shape[2]\n",
    "\n",
    "    with h5py.File(path, \"w\") as f:\n",
    "        X_ds = f.create_dataset(\n",
    "            \"X\",\n",
    "            shape=(n, WINDOW_SIZE, F),\n",
    "            dtype=\"float32\",\n",
    "            compression=\"gzip\",\n",
    "            chunks=(min(BATCH_SIZE, n), WINDOW_SIZE, F),\n",
    "        )\n",
    "        y_ds = f.create_dataset(\n",
    "            \"y\",\n",
    "            shape=(n, 2),\n",
    "            dtype=\"float32\",\n",
    "            compression=\"gzip\",\n",
    "            chunks=(min(BATCH_SIZE, n), 2),\n",
    "        )\n",
    "        sym_ds = f.create_dataset(\"symbol\", shape=(n,), dtype=\"S16\")\n",
    "\n",
    "        # ‚úÖ FIX: int64 nanoseconds since epoch\n",
    "        date_ds = f.create_dataset(\"date\", shape=(n,), dtype=\"int64\")\n",
    "\n",
    "        for start in range(0, n, BATCH_SIZE):\n",
    "            end = min(start + BATCH_SIZE, n)\n",
    "            batch_idx = indices[start:end]\n",
    "\n",
    "            X_ds[start:end] = X[batch_idx]\n",
    "            y_ds[start:end] = y[batch_idx]\n",
    "            sym_ds[start:end] = np.array(\n",
    "                [meta[i][0] for i in batch_idx], dtype=\"S16\"\n",
    "            )\n",
    "            date_ds[start:end] = np.array(\n",
    "                [meta[i][1].astype(\"datetime64[ns]\").astype(\"int64\") for i in batch_idx]\n",
    "            )\n",
    "\n",
    "            if start == 0 or end == n:\n",
    "                print(f\"  wrote rows {start} ‚Üí {end}\")\n",
    "\n",
    "\n",
    "# ---- Write files ----\n",
    "write_h5_chunked(\"gnn_sequences_train.h5\", train_idx)\n",
    "write_h5_chunked(\"gnn_sequences_val.h5\", val_idx)\n",
    "write_h5_chunked(\"gnn_sequences_test.h5\", test_idx)\n",
    "\n",
    "print(\"‚úÖ HDF5 files written successfully (datetime stored as int64 ns)\")\n",
    "print(\"\\nCell 9 PASSED ‚Äî Memory-safe persistence complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39b0addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ node_features.parquet written\n",
      "Rows: 257794\n",
      "Features per node: 263\n",
      "Path: D:\\MarketSentinel\\MarketSentinel\\node_features.parquet\n",
      "\n",
      "Cell 10 PASSED ‚Äî Node features materialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Write node_features.parquet\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Final safety check\n",
    "features = df[feature_cols]\n",
    "\n",
    "if not np.isfinite(features.values).all():\n",
    "    raise ValueError(\"‚ùå Contract violation: Non-finite values in node features\")\n",
    "\n",
    "if features.shape[1] != len(feature_cols):\n",
    "    raise ValueError(\"‚ùå Feature dimension mismatch with frozen schema\")\n",
    "\n",
    "node_df = pd.concat(\n",
    "    [\n",
    "        df[[\"symbol\", \"date\"]].reset_index(drop=True),\n",
    "        features.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "out_path = Path(\"node_features.parquet\")\n",
    "node_df.to_parquet(out_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ node_features.parquet written\")\n",
    "print(f\"Rows: {len(node_df)}\")\n",
    "print(f\"Features per node: {len(feature_cols)}\")\n",
    "print(f\"Path: {out_path.resolve()}\")\n",
    "\n",
    "print(\"\\nCell 10 PASSED ‚Äî Node features materialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d60b0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema feature count: 263\n",
      "‚úÖ train split OK ‚Äî 177703 samples\n",
      "‚úÖ val split OK ‚Äî 38048 samples\n",
      "‚úÖ test split OK ‚Äî 38149 samples\n",
      "‚úÖ node_features.parquet OK\n",
      "\n",
      "üéâ DATA PRECHECK PASSED ‚Äî DATASET IS TRAIN-READY üéâ\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Data Precheck (Compiler Gate)\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Load frozen schema ----\n",
    "schema_path = Path(\"feature_schema.txt\")\n",
    "if not schema_path.exists():\n",
    "    raise FileNotFoundError(\"‚ùå Missing feature_schema.txt\")\n",
    "\n",
    "with open(schema_path) as f:\n",
    "    schema_features = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "F = len(schema_features)\n",
    "print(f\"Schema feature count: {F}\")\n",
    "\n",
    "# ---- Check HDF5 files ----\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    path = Path(f\"gnn_sequences_{split}.h5\")\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Missing {path}\")\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        X = f[\"X\"]\n",
    "        y = f[\"y\"]\n",
    "        symbols = f[\"symbol\"]\n",
    "        dates = f[\"date\"]\n",
    "\n",
    "        assert X.ndim == 3 and X.shape[1] == 60 and X.shape[2] == F\n",
    "        assert y.shape[1] == 2\n",
    "        assert X.dtype == np.float32\n",
    "        assert y.dtype == np.float32\n",
    "        assert dates.dtype == np.int64\n",
    "\n",
    "        if not np.isfinite(X[:1000]).all():\n",
    "            raise ValueError(f\"‚ùå Non-finite values in X ({split})\")\n",
    "        if not np.isfinite(y[:1000]).all():\n",
    "            raise ValueError(f\"‚ùå Non-finite values in y ({split})\")\n",
    "\n",
    "        # Date round-trip check\n",
    "        _ = dates[:10].astype(\"datetime64[ns]\")\n",
    "\n",
    "        print(f\"‚úÖ {split} split OK ‚Äî {X.shape[0]} samples\")\n",
    "\n",
    "# ---- Check node_features.parquet ----\n",
    "node_df = pd.read_parquet(\"node_features.parquet\")\n",
    "\n",
    "expected_cols = [\"symbol\", \"date\"] + schema_features\n",
    "if list(node_df.columns) != expected_cols:\n",
    "    raise ValueError(\"‚ùå node_features column order mismatch\")\n",
    "\n",
    "if not np.isfinite(node_df[schema_features].values).all():\n",
    "    raise ValueError(\"‚ùå Non-finite values in node_features\")\n",
    "\n",
    "print(\"‚úÖ node_features.parquet OK\")\n",
    "\n",
    "print(\"\\nüéâ DATA PRECHECK PASSED ‚Äî DATASET IS TRAIN-READY üéâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c79bffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building static edges for 66 symbols\n",
      "‚úÖ edges_static.parquet written\n",
      "Edges: 66\n",
      "Path: D:\\MarketSentinel\\MarketSentinel\\edges_static.parquet\n",
      "\n",
      "Cell 12 PASSED ‚Äî Static edge graph created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Build edges_static.parquet (baseline self-loop graph)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Get symbol universe from node features ----\n",
    "node_df = pd.read_parquet(\"node_features.parquet\")\n",
    "symbols = sorted(node_df[\"symbol\"].unique())\n",
    "\n",
    "if len(symbols) == 0:\n",
    "    raise ValueError(\"‚ùå No symbols found in node_features\")\n",
    "\n",
    "print(f\"Building static edges for {len(symbols)} symbols\")\n",
    "\n",
    "# ---- Self-loop edges ----\n",
    "edges = pd.DataFrame({\n",
    "    \"symbol_i\": symbols,\n",
    "    \"symbol_j\": symbols,\n",
    "    \"weight\": np.ones(len(symbols), dtype=np.float32),\n",
    "})\n",
    "\n",
    "# ---- Validation ----\n",
    "if not np.isfinite(edges[\"weight\"].values).all():\n",
    "    raise ValueError(\"‚ùå Non-finite edge weights\")\n",
    "\n",
    "if not set(edges[\"symbol_i\"]).issubset(symbols):\n",
    "    raise ValueError(\"‚ùå symbol_i outside node_features symbol set\")\n",
    "\n",
    "if not set(edges[\"symbol_j\"]).issubset(symbols):\n",
    "    raise ValueError(\"‚ùå symbol_j outside node_features symbol set\")\n",
    "\n",
    "# ---- Write file ----\n",
    "out_path = Path(\"edges_static.parquet\")\n",
    "edges.to_parquet(out_path, index=False)\n",
    "\n",
    "print(\"‚úÖ edges_static.parquet written\")\n",
    "print(f\"Edges: {len(edges)}\")\n",
    "print(f\"Path: {out_path.resolve()}\")\n",
    "\n",
    "print(\"\\nCell 12 PASSED ‚Äî Static edge graph created.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
