{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb3927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç MarketSentinel ‚Äî Data Contract Validation\n",
      "üìÇ Running Cell 1: Dataset inventory & path sanity\n",
      "\n",
      "Project root:   d:\\MarketSentinel\\MarketSentinel\n",
      "Datasets path:  d:\\MarketSentinel\\MarketSentinel\\datasets\n",
      "\n",
      "‚úÖ Required dataset files present:\n",
      "  - feature_schema.txt\n",
      "  - node_features.parquet\n",
      "  - edges_static.parquet\n",
      "  - gnn_sequences_train.h5\n",
      "  - gnn_sequences_val.h5\n",
      "  - gnn_sequences_test.h5\n",
      "\n",
      "‚úÖ Cell 1 PASSED ‚Äî Dataset presence verified.\n",
      "‚û°Ô∏è Proceed to Cell 2: feature_schema.txt validation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Cell 1: Dataset Inventory & Path Sanity (Memory-Safe)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç MarketSentinel ‚Äî Data Contract Validation\")\n",
    "print(\"üìÇ Running Cell 1: Dataset inventory & path sanity\\n\")\n",
    "\n",
    "# PROJECT_ROOT = Path(\"D:/MarketSentinel/MarketSentinel\")  # or absolute path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "print(f\"Project root:   {PROJECT_ROOT}\")\n",
    "print(f\"Datasets path:  {DATASET_DIR}\\n\")\n",
    "\n",
    "# ---- Hard assertions ----\n",
    "assert PROJECT_ROOT.exists(), \"‚ùå Project root does not exist\"\n",
    "assert DATASET_DIR.exists(), \"‚ùå datasets/ directory is missing\"\n",
    "\n",
    "REQUIRED_FILES = [\n",
    "    \"feature_schema.txt\",\n",
    "    \"node_features.parquet\",\n",
    "    \"edges_static.parquet\",\n",
    "    \"gnn_sequences_train.h5\",\n",
    "    \"gnn_sequences_val.h5\",\n",
    "    \"gnn_sequences_test.h5\",\n",
    "]\n",
    "\n",
    "missing = [f for f in REQUIRED_FILES if not (DATASET_DIR / f).exists()]\n",
    "assert not missing, f\"‚ùå Missing required dataset files: {missing}\"\n",
    "\n",
    "print(\"‚úÖ Required dataset files present:\")\n",
    "for f in REQUIRED_FILES:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 1 PASSED ‚Äî Dataset presence verified.\")\n",
    "print(\"‚û°Ô∏è Proceed to Cell 2: feature_schema.txt validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6560ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 2: feature_schema.txt validation\n",
      "\n",
      "‚úÖ feature_schema.txt loaded successfully\n",
      "üî¢ Total features: 263\n",
      "üìå First 5 features:\n",
      "  - atr_14\n",
      "  - atr_14_rmean_60\n",
      "  - atr_14_rstd_60\n",
      "  - atr_14_rz_60\n",
      "  - close_z_20\n",
      "üìå Last 5 features:\n",
      "  - volume_z_20\n",
      "  - volume_z_20_rmean_60\n",
      "  - volume_z_20_rstd_60\n",
      "  - volume_z_20_rz_60\n",
      "  - year\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: feature_schema.txt validation\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üß™ Running Cell 2: feature_schema.txt validation\\n\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Load schema ----\n",
    "assert SCHEMA_PATH.exists(), \"‚ùå feature_schema.txt does not exist\"\n",
    "\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    raw_features = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "# ---- Hard validations ----\n",
    "assert len(raw_features) > 0, \"‚ùå feature_schema.txt is empty\"\n",
    "\n",
    "# No empty feature names\n",
    "empty_features = [i for i, f in enumerate(raw_features) if f.strip() == \"\"]\n",
    "assert not empty_features, f\"‚ùå Empty feature names at lines: {empty_features}\"\n",
    "\n",
    "# No leading/trailing whitespace\n",
    "whitespace_issues = [f for f in raw_features if f != f.strip()]\n",
    "assert not whitespace_issues, (\n",
    "    \"‚ùå Feature names contain leading/trailing whitespace: \"\n",
    "    f\"{whitespace_issues}\"\n",
    ")\n",
    "\n",
    "# Uniqueness\n",
    "duplicates = sorted(set([f for f in raw_features if raw_features.count(f) > 1]))\n",
    "assert not duplicates, f\"‚ùå Duplicate feature names found: {duplicates}\"\n",
    "\n",
    "# ---- Final report ----\n",
    "num_features = len(raw_features)\n",
    "\n",
    "print(f\"‚úÖ feature_schema.txt loaded successfully\")\n",
    "print(f\"üî¢ Total features: {num_features}\")\n",
    "print(\"üìå First 5 features:\")\n",
    "for f in raw_features[:5]:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(\"üìå Last 5 features:\")\n",
    "for f in raw_features[-5:]:\n",
    "    print(f\"  - {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f286bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ† Fixing node_features.parquet dtypes ‚Üí float32\n",
      "\n",
      "‚úÖ node_features.parquet rewritten with float32 features\n",
      "‚û°Ô∏è Re-run Cell 3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3A: FIX ‚Äî enforce float32 on node_features.parquet\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üõ† Fixing node_features.parquet dtypes ‚Üí float32\\n\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "NODE_FEATURES_PATH = DATASET_DIR / \"node_features.parquet\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# Load schema\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "# Load parquet\n",
    "df = pd.read_parquet(NODE_FEATURES_PATH)\n",
    "\n",
    "# Cast features explicitly\n",
    "for col in feature_schema:\n",
    "    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "# Safety check\n",
    "for col in feature_schema:\n",
    "    assert df[col].dtype == np.float32, f\"‚ùå Failed to cast {col} to float32\"\n",
    "\n",
    "# Rewrite parquet (overwrite is intentional)\n",
    "df.to_parquet(NODE_FEATURES_PATH, index=False)\n",
    "\n",
    "print(\"‚úÖ node_features.parquet rewritten with float32 features\")\n",
    "print(\"‚û°Ô∏è Re-run Cell 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8919538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 3: node_features.parquet validation\n",
      "\n",
      "‚úÖ node_features.parquet structure is valid\n",
      "üìê Total rows: 257794\n",
      "üî¢ Feature count: 263\n",
      "üîó Unique symbols (66): ['AAPL', 'ABBV', 'AMZN', 'ASML', 'BA']...\n",
      "\n",
      "‚úÖ Cell 3 PASSED ‚Äî Node feature table conforms to contract.\n",
      "‚û°Ô∏è Proceed to Cell 4: edges_static.parquet validation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3: node_features.parquet validation (memory-safe)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üß™ Running Cell 3: node_features.parquet validation\\n\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "NODE_FEATURES_PATH = DATASET_DIR / \"node_features.parquet\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "assert NODE_FEATURES_PATH.exists(), \"‚ùå node_features.parquet does not exist\"\n",
    "assert SCHEMA_PATH.exists(), \"‚ùå feature_schema.txt does not exist\"\n",
    "\n",
    "# ---- Load feature schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "F = len(feature_schema)\n",
    "assert F > 0, \"‚ùå Feature schema is empty (should not happen if Cell 2 passed)\"\n",
    "\n",
    "# ---- Load parquet (column metadata first) ----\n",
    "df = pd.read_parquet(NODE_FEATURES_PATH)\n",
    "\n",
    "# ---- Column order validation ----\n",
    "expected_columns = [\"symbol\", \"date\"] + feature_schema\n",
    "actual_columns = list(df.columns)\n",
    "\n",
    "assert actual_columns == expected_columns, (\n",
    "    \"‚ùå Column order mismatch in node_features.parquet\\n\"\n",
    "    f\"Expected: {expected_columns[:5]} ...\\n\"\n",
    "    f\"Actual:   {actual_columns[:5]} ...\"\n",
    ")\n",
    "\n",
    "# ---- Dtype validation ----\n",
    "assert df[\"symbol\"].dtype == object, \"‚ùå 'symbol' column must be string/object\"\n",
    "assert np.issubdtype(df[\"date\"].dtype, np.datetime64), (\n",
    "    \"‚ùå 'date' column must be datetime64\"\n",
    ")\n",
    "\n",
    "for col in feature_schema:\n",
    "    assert df[col].dtype == np.float32, (\n",
    "        f\"‚ùå Feature '{col}' has dtype {df[col].dtype}, expected float32\"\n",
    "    )\n",
    "\n",
    "# ---- Finiteness validation (features only) ----\n",
    "feature_values = df[feature_schema].to_numpy()\n",
    "assert np.isfinite(feature_values).all(), \"‚ùå Non-finite values found in features\"\n",
    "\n",
    "# ---- Symbol sanity ----\n",
    "assert df[\"symbol\"].isnull().sum() == 0, \"‚ùå Null symbols found\"\n",
    "unique_symbols = sorted(df[\"symbol\"].unique())\n",
    "\n",
    "# ---- Final report ----\n",
    "print(\"‚úÖ node_features.parquet structure is valid\")\n",
    "print(f\"üìê Total rows: {len(df)}\")\n",
    "print(f\"üî¢ Feature count: {F}\")\n",
    "print(f\"üîó Unique symbols ({len(unique_symbols)}): {unique_symbols[:5]}{'...' if len(unique_symbols) > 5 else ''}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 3 PASSED ‚Äî Node feature table conforms to contract.\")\n",
    "print(\"‚û°Ô∏è Proceed to Cell 4: edges_static.parquet validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3872ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 4: edges_static.parquet validation\n",
      "\n",
      "‚úÖ edges_static.parquet structure and semantics are valid\n",
      "üîó Total edges: 66\n",
      "üß© Identity graph confirmed (self-loops only)\n",
      "\n",
      "‚úÖ Cell 4 PASSED ‚Äî Graph contract satisfied.\n",
      "‚û°Ô∏è Proceed to Cell 5: HDF5 temporal sequence validation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 4: edges_static.parquet validation\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üß™ Running Cell 4: edges_static.parquet validation\\n\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "EDGES_PATH = DATASET_DIR / \"edges_static.parquet\"\n",
    "NODE_FEATURES_PATH = DATASET_DIR / \"node_features.parquet\"\n",
    "\n",
    "assert EDGES_PATH.exists(), \"‚ùå edges_static.parquet does not exist\"\n",
    "assert NODE_FEATURES_PATH.exists(), \"‚ùå node_features.parquet does not exist\"\n",
    "\n",
    "# ---- Load edges ----\n",
    "edges = pd.read_parquet(EDGES_PATH)\n",
    "\n",
    "required_cols = [\"symbol_i\", \"symbol_j\", \"weight\"]\n",
    "assert list(edges.columns) == required_cols, (\n",
    "    f\"‚ùå edges_static.parquet columns must be {required_cols}, \"\n",
    "    f\"found {list(edges.columns)}\"\n",
    ")\n",
    "\n",
    "# ---- Dtype validation ----\n",
    "assert edges[\"symbol_i\"].dtype == object, \"‚ùå symbol_i must be string/object\"\n",
    "assert edges[\"symbol_j\"].dtype == object, \"‚ùå symbol_j must be string/object\"\n",
    "assert edges[\"weight\"].dtype == np.float32, \"‚ùå weight must be float32\"\n",
    "\n",
    "# ---- Finiteness ----\n",
    "assert np.isfinite(edges[\"weight\"].to_numpy()).all(), \"‚ùå Non-finite edge weights found\"\n",
    "\n",
    "# ---- Load node symbols for alignment ----\n",
    "node_df = pd.read_parquet(NODE_FEATURES_PATH, columns=[\"symbol\"])\n",
    "node_symbols = set(node_df[\"symbol\"].unique())\n",
    "\n",
    "edge_symbols_i = set(edges[\"symbol_i\"].unique())\n",
    "edge_symbols_j = set(edges[\"symbol_j\"].unique())\n",
    "\n",
    "assert edge_symbols_i.issubset(node_symbols), (\n",
    "    f\"‚ùå symbol_i contains unknown symbols: {edge_symbols_i - node_symbols}\"\n",
    ")\n",
    "assert edge_symbols_j.issubset(node_symbols), (\n",
    "    f\"‚ùå symbol_j contains unknown symbols: {edge_symbols_j - node_symbols}\"\n",
    ")\n",
    "\n",
    "# ---- Baseline identity-graph semantics (v1 contract) ----\n",
    "non_identity = edges[edges[\"symbol_i\"] != edges[\"symbol_j\"]]\n",
    "assert non_identity.empty, (\n",
    "    \"‚ùå Non-identity edges found. \"\n",
    "    \"Baseline graph must contain only self-loops.\"\n",
    ")\n",
    "\n",
    "non_unit_weight = edges[edges[\"weight\"] != 1.0]\n",
    "assert non_unit_weight.empty, (\n",
    "    \"‚ùå Edge weights must be exactly 1.0 for identity graph.\"\n",
    ")\n",
    "\n",
    "# ---- Final report ----\n",
    "print(\"‚úÖ edges_static.parquet structure and semantics are valid\")\n",
    "print(f\"üîó Total edges: {len(edges)}\")\n",
    "print(f\"üß© Identity graph confirmed (self-loops only)\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 4 PASSED ‚Äî Graph contract satisfied.\")\n",
    "print(\"‚û°Ô∏è Proceed to Cell 5: HDF5 temporal sequence validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea2ee080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 5: HDF5 temporal sequence validation\n",
      "\n",
      "‚úÖ train: OK | N=177703, T=60, F=263\n",
      "‚úÖ val: OK | N=38048, T=60, F=263\n",
      "‚úÖ test: OK | N=38149, T=60, F=263\n",
      "\n",
      "‚úÖ Cell 5 PASSED ‚Äî All HDF5 temporal datasets satisfy the contract.\n",
      "‚û°Ô∏è Proceed to Cell 6: Cross-artifact consistency checks\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: HDF5 temporal sequence validation (lazy, memory-safe)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üß™ Running Cell 5: HDF5 temporal sequence validation\\n\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Load schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "F = len(feature_schema)\n",
    "SEQ_LEN = 60\n",
    "TARGET_DIM = 2\n",
    "\n",
    "H5_FILES = {\n",
    "    \"train\": DATASET_DIR / \"gnn_sequences_train.h5\",\n",
    "    \"val\":   DATASET_DIR / \"gnn_sequences_val.h5\",\n",
    "    \"test\":  DATASET_DIR / \"gnn_sequences_test.h5\",\n",
    "}\n",
    "\n",
    "def validate_h5(split, path, sample_n=8):\n",
    "    assert path.exists(), f\"‚ùå {split}: HDF5 file missing\"\n",
    "\n",
    "    with h5py.File(path, \"r\") as h5:\n",
    "        # ---- Required datasets ----\n",
    "        for key in [\"X\", \"y\", \"symbol\", \"date\"]:\n",
    "            assert key in h5, f\"‚ùå {split}: Missing dataset '{key}'\"\n",
    "\n",
    "        X = h5[\"X\"]\n",
    "        y = h5[\"y\"]\n",
    "        sym = h5[\"symbol\"]\n",
    "        date = h5[\"date\"]\n",
    "\n",
    "        # ---- Shape checks ----\n",
    "        assert X.ndim == 3, f\"‚ùå {split}: X must be 3D\"\n",
    "        N, T, Fh5 = X.shape\n",
    "        assert T == SEQ_LEN, f\"‚ùå {split}: sequence length {T} != {SEQ_LEN}\"\n",
    "        assert Fh5 == F, f\"‚ùå {split}: feature dim {Fh5} != {F}\"\n",
    "\n",
    "        assert y.shape == (N, TARGET_DIM), f\"‚ùå {split}: y shape {y.shape} != (N, {TARGET_DIM})\"\n",
    "        assert sym.shape == (N,), f\"‚ùå {split}: symbol shape invalid\"\n",
    "        assert date.shape == (N,), f\"‚ùå {split}: date shape invalid\"\n",
    "\n",
    "        # ---- Dtype checks ----\n",
    "        assert X.dtype == np.float32, f\"‚ùå {split}: X dtype {X.dtype} != float32\"\n",
    "        assert y.dtype == np.float32, f\"‚ùå {split}: y dtype {y.dtype} != float32\"\n",
    "        assert date.dtype == np.int64, f\"‚ùå {split}: date dtype {date.dtype} != int64\"\n",
    "\n",
    "        # ---- Sampled finiteness checks (avoid full load) ----\n",
    "        if N > 0:\n",
    "            idx = np.linspace(0, N - 1, min(sample_n, N)).astype(int)\n",
    "            X_sample = X[idx, :, :]\n",
    "            y_sample = y[idx, :]\n",
    "            assert np.isfinite(X_sample).all(), f\"‚ùå {split}: Non-finite values in X sample\"\n",
    "            assert np.isfinite(y_sample).all(), f\"‚ùå {split}: Non-finite values in y sample\"\n",
    "\n",
    "        # ---- Date reversibility ----\n",
    "        try:\n",
    "            _ = pd.to_datetime(date[:min(5, N)], unit=\"ns\")\n",
    "        except Exception as e:\n",
    "            raise AssertionError(f\"‚ùå {split}: date not convertible to datetime\") from e\n",
    "\n",
    "        print(f\"‚úÖ {split}: OK | N={N}, T={T}, F={Fh5}\")\n",
    "\n",
    "# ---- Run validation for each split ----\n",
    "for split, path in H5_FILES.items():\n",
    "    validate_h5(split, path)\n",
    "\n",
    "print(\"\\n‚úÖ Cell 5 PASSED ‚Äî All HDF5 temporal datasets satisfy the contract.\")\n",
    "print(\"‚û°Ô∏è Proceed to Cell 6: Cross-artifact consistency checks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20fd692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 6: Cross-artifact consistency checks\n",
      "\n",
      "üîó Node symbols: 66\n",
      "‚úÖ train: symbols=66, samples=177703\n",
      "‚úÖ val: symbols=66, samples=38048\n",
      "‚úÖ test: symbols=66, samples=38149\n",
      "\n",
      "‚úÖ All splits share identical symbol universe\n",
      "\n",
      "‚úÖ Cell 6 PASSED ‚Äî Cross-artifact consistency confirmed.\n",
      "‚û°Ô∏è Proceed to Cell 7: FINAL UNLOCK\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Cross-artifact consistency validation\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "print(\"üß™ Running Cell 6: Cross-artifact consistency checks\\n\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "# ---- Paths ----\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "NODE_FEATURES_PATH = DATASET_DIR / \"node_features.parquet\"\n",
    "H5_PATHS = {\n",
    "    \"train\": DATASET_DIR / \"gnn_sequences_train.h5\",\n",
    "    \"val\":   DATASET_DIR / \"gnn_sequences_val.h5\",\n",
    "    \"test\":  DATASET_DIR / \"gnn_sequences_test.h5\",\n",
    "}\n",
    "\n",
    "# ---- Load schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "F = len(feature_schema)\n",
    "\n",
    "# ---- Load node feature symbols ----\n",
    "node_df = pd.read_parquet(NODE_FEATURES_PATH, columns=[\"symbol\"])\n",
    "node_symbols = set(node_df[\"symbol\"].unique())\n",
    "\n",
    "assert len(node_symbols) > 0, \"‚ùå No symbols found in node_features.parquet\"\n",
    "\n",
    "print(f\"üîó Node symbols: {len(node_symbols)}\")\n",
    "\n",
    "# ---- Validate each HDF5 split ----\n",
    "split_symbols = {}\n",
    "\n",
    "for split, path in H5_PATHS.items():\n",
    "    with h5py.File(path, \"r\") as h5:\n",
    "        X = h5[\"X\"]\n",
    "        sym = h5[\"symbol\"]\n",
    "\n",
    "        # Feature dimension consistency\n",
    "        assert X.shape[2] == F, (\n",
    "            f\"‚ùå {split}: Feature dimension {X.shape[2]} != schema {F}\"\n",
    "        )\n",
    "\n",
    "        # Symbol consistency\n",
    "        symbols_in_split = set(s.decode(\"utf-8\") if isinstance(s, bytes) else s for s in sym[:])\n",
    "        unknown = symbols_in_split - node_symbols\n",
    "        assert not unknown, (\n",
    "            f\"‚ùå {split}: Unknown symbols found: {unknown}\"\n",
    "        )\n",
    "\n",
    "        split_symbols[split] = symbols_in_split\n",
    "\n",
    "        print(f\"‚úÖ {split}: symbols={len(symbols_in_split)}, samples={X.shape[0]}\")\n",
    "\n",
    "# ---- Split isolation check (no leakage) ----\n",
    "train_syms = split_symbols[\"train\"]\n",
    "val_syms   = split_symbols[\"val\"]\n",
    "test_syms  = split_symbols[\"test\"]\n",
    "\n",
    "assert train_syms == val_syms == test_syms, (\n",
    "    \"‚ùå Symbol sets differ across splits ‚Äî potential leakage or filtering bug\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All splits share identical symbol universe\")\n",
    "\n",
    "print(\"\\n‚úÖ Cell 6 PASSED ‚Äî Cross-artifact consistency confirmed.\")\n",
    "print(\"‚û°Ô∏è Proceed to Cell 7: FINAL UNLOCK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b90d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ MARKET SENTINEL ‚Äî DATA CONTRACTS SATISFIED\n",
      "============================================\n",
      "Timestamp: 2026-02-05T08:50:19.012138Z\n",
      "\n",
      "‚úÖ feature_schema.txt        ‚Äî VALID\n",
      "‚úÖ node_features.parquet     ‚Äî VALID\n",
      "‚úÖ edges_static.parquet      ‚Äî VALID\n",
      "‚úÖ gnn_sequences_train.h5    ‚Äî VALID\n",
      "‚úÖ gnn_sequences_val.h5      ‚Äî VALID\n",
      "‚úÖ gnn_sequences_test.h5     ‚Äî VALID\n",
      "\n",
      "üîí All data interfaces are consistent, finite, and schema-aligned.\n",
      "üö´ No preprocessing, inference, or mutation permitted downstream.\n",
      "‚û°Ô∏è DataLoader, Model, Training, and Backtesting are now AUTHORIZED.\n",
      "\n",
      "üèÅ DATA COMPILATION COMPLETE ‚Äî TRAINING UNLOCKED\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 7: FINAL UNLOCK ‚Äî Data Contracts Satisfied\n",
    "# ============================================================\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üü¢ MARKET SENTINEL ‚Äî DATA CONTRACTS SATISFIED\")\n",
    "print(\"============================================\")\n",
    "print(f\"Timestamp: {datetime.utcnow().isoformat()}Z\\n\")\n",
    "\n",
    "print(\"‚úÖ feature_schema.txt        ‚Äî VALID\")\n",
    "print(\"‚úÖ node_features.parquet     ‚Äî VALID\")\n",
    "print(\"‚úÖ edges_static.parquet      ‚Äî VALID\")\n",
    "print(\"‚úÖ gnn_sequences_train.h5    ‚Äî VALID\")\n",
    "print(\"‚úÖ gnn_sequences_val.h5      ‚Äî VALID\")\n",
    "print(\"‚úÖ gnn_sequences_test.h5     ‚Äî VALID\\n\")\n",
    "\n",
    "print(\"üîí All data interfaces are consistent, finite, and schema-aligned.\")\n",
    "print(\"üö´ No preprocessing, inference, or mutation permitted downstream.\")\n",
    "print(\"‚û°Ô∏è DataLoader, Model, Training, and Backtesting are now AUTHORIZED.\")\n",
    "\n",
    "print(\"\\nüèÅ DATA COMPILATION COMPLETE ‚Äî TRAINING UNLOCKED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "853c55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 8: DataLoader smoke test\n",
      "\n",
      "‚úÖ Dataset instantiated | length = 177703\n",
      "üìå Sample inspection:\n",
      "  Symbol: AAPL\n",
      "  Date:   2015-03-02 00:00:00\n",
      "  X shape: (60, 263) | y: [0.002091564005240798, 0.0]\n",
      "\n",
      "‚úÖ Cell 8 PASSED ‚Äî DataLoader is contract-compliant.\n",
      "‚û°Ô∏è Proceed to Model definition phase\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 8: DataLoader smoke test (single sample)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# ---- Ensure project root is on PYTHONPATH ----\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from dataloader.temporal_dataloader import TemporalSequenceDataset\n",
    "\n",
    "print(\"üß™ Running Cell 8: DataLoader smoke test\\n\")\n",
    "\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "H5_PATH = DATASET_DIR / \"gnn_sequences_train.h5\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Instantiate dataset ----\n",
    "dataset = TemporalSequenceDataset(\n",
    "    h5_path=H5_PATH,\n",
    "    feature_schema_path=SCHEMA_PATH,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset instantiated | length = {len(dataset)}\")\n",
    "\n",
    "# ---- Pull one sample ----\n",
    "sample = dataset[0]\n",
    "\n",
    "X = sample[\"X\"]\n",
    "y = sample[\"y\"]\n",
    "symbol = sample[\"symbol\"]\n",
    "date = sample[\"date\"]\n",
    "\n",
    "# ---- Assertions ----\n",
    "assert isinstance(X, torch.Tensor), \"‚ùå X is not a torch.Tensor\"\n",
    "assert isinstance(y, torch.Tensor), \"‚ùå y is not a torch.Tensor\"\n",
    "\n",
    "assert X.shape[0] == 60, \"‚ùå X sequence length != 60\"\n",
    "assert X.ndim == 2, \"‚ùå X must be [60, F]\"\n",
    "assert y.shape == (2,), \"‚ùå y must be shape (2,)\"\n",
    "\n",
    "assert X.dtype == torch.float32, \"‚ùå X dtype != float32\"\n",
    "assert y.dtype == torch.float32, \"‚ùå y dtype != float32\"\n",
    "\n",
    "assert isinstance(symbol, str), \"‚ùå symbol must be str\"\n",
    "assert hasattr(date, \"to_pydatetime\"), \"‚ùå date must be datetime-like\"\n",
    "\n",
    "print(\"üìå Sample inspection:\")\n",
    "print(f\"  Symbol: {symbol}\")\n",
    "print(f\"  Date:   {date}\")\n",
    "print(f\"  X shape: {tuple(X.shape)} | y: {y.tolist()}\")\n",
    "\n",
    "# ---- Cleanup ----\n",
    "dataset.close()\n",
    "\n",
    "print(\"\\n‚úÖ Cell 8 PASSED ‚Äî DataLoader is contract-compliant.\")\n",
    "print(\"‚û°Ô∏è Proceed to Model definition phase\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83ebcd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 9: TemporalEncoder smoke test\n",
      "\n",
      "üìê Input batch shape: (4, 60, 263)\n",
      "üìå TemporalEncoder output inspection:\n",
      "  Output shape: (4, 128)\n",
      "  Dtype: torch.float32\n",
      "  Sample (first row, first 5 vals): [-0.9228795170783997, 0.9436871409416199, 0.7474874258041382, -0.42305558919906616, 0.9453398585319519]\n",
      "\n",
      "‚úÖ Cell 9 PASSED ‚Äî TemporalEncoder is contract-compliant.\n",
      "‚û°Ô∏è Proceed to GraphEncoder implementation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 9: TemporalEncoder smoke test\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# ---- Ensure project root on PYTHONPATH ----\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from dataloader.temporal_dataloader import TemporalSequenceDataset\n",
    "from models.temporal_encoder import TemporalEncoder\n",
    "\n",
    "print(\"üß™ Running Cell 9: TemporalEncoder smoke test\\n\")\n",
    "\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "H5_PATH = DATASET_DIR / \"gnn_sequences_train.h5\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Load feature schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "F = len(feature_schema)\n",
    "H = 128  # hidden dimension for smoke test (arbitrary but fixed)\n",
    "\n",
    "# ---- Instantiate dataset and pull a mini-batch ----\n",
    "dataset = TemporalSequenceDataset(\n",
    "    h5_path=H5_PATH,\n",
    "    feature_schema_path=SCHEMA_PATH,\n",
    ")\n",
    "\n",
    "# Create a small batch manually (no DataLoader yet)\n",
    "batch_size = 4\n",
    "X_batch = torch.stack([dataset[i][\"X\"] for i in range(batch_size)], dim=0)\n",
    "\n",
    "print(f\"üìê Input batch shape: {tuple(X_batch.shape)}\")\n",
    "\n",
    "# ---- Instantiate TemporalEncoder ----\n",
    "encoder = TemporalEncoder(\n",
    "    input_dim=F,\n",
    "    hidden_dim=H,\n",
    ")\n",
    "\n",
    "# ---- Forward pass ----\n",
    "with torch.no_grad():\n",
    "    H_out = encoder(X_batch)\n",
    "\n",
    "# ---- Assertions ----\n",
    "assert isinstance(H_out, torch.Tensor), \"‚ùå Output is not a torch.Tensor\"\n",
    "assert H_out.shape == (batch_size, H), f\"‚ùå Output shape {H_out.shape} != {(batch_size, H)}\"\n",
    "assert H_out.dtype == torch.float32, \"‚ùå Output dtype != float32\"\n",
    "assert torch.isfinite(H_out).all(), \"‚ùå Non-finite values in encoder output\"\n",
    "\n",
    "print(\"üìå TemporalEncoder output inspection:\")\n",
    "print(f\"  Output shape: {tuple(H_out.shape)}\")\n",
    "print(f\"  Dtype: {H_out.dtype}\")\n",
    "print(f\"  Sample (first row, first 5 vals): {H_out[0, :5].tolist()}\")\n",
    "\n",
    "# ---- Cleanup ----\n",
    "dataset.close()\n",
    "\n",
    "print(\"\\n‚úÖ Cell 9 PASSED ‚Äî TemporalEncoder is contract-compliant.\")\n",
    "print(\"‚û°Ô∏è Proceed to GraphEncoder implementation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd28e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 10: GraphEncoder smoke test\n",
      "\n",
      "üìê Temporal embedding shape: (4, 128)\n",
      "üìå GraphEncoder output inspection:\n",
      "  Output shape: (4, 128)\n",
      "  Identity mapping confirmed\n",
      "\n",
      "‚úÖ Cell 10 PASSED ‚Äî GraphEncoder is contract-compliant.\n",
      "‚û°Ô∏è Proceed to FusionLayer implementation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 10: GraphEncoder smoke test\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# ---- Ensure project root on PYTHONPATH ----\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from dataloader.temporal_dataloader import TemporalSequenceDataset\n",
    "from models.temporal_encoder import TemporalEncoder\n",
    "from models.graph_encoder import GraphEncoder\n",
    "\n",
    "print(\"üß™ Running Cell 10: GraphEncoder smoke test\\n\")\n",
    "\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "H5_PATH = DATASET_DIR / \"gnn_sequences_train.h5\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Load schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "F = len(feature_schema)\n",
    "H = 128\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# ---- Dataset + batch ----\n",
    "dataset = TemporalSequenceDataset(\n",
    "    h5_path=H5_PATH,\n",
    "    feature_schema_path=SCHEMA_PATH,\n",
    ")\n",
    "\n",
    "X_batch = torch.stack([dataset[i][\"X\"] for i in range(BATCH_SIZE)], dim=0)\n",
    "\n",
    "# ---- Temporal encoding ----\n",
    "temporal_encoder = TemporalEncoder(input_dim=F, hidden_dim=H)\n",
    "with torch.no_grad():\n",
    "    H_temporal = temporal_encoder(X_batch)\n",
    "\n",
    "print(f\"üìê Temporal embedding shape: {tuple(H_temporal.shape)}\")\n",
    "\n",
    "# ---- Graph encoding ----\n",
    "graph_encoder = GraphEncoder(hidden_dim=H)\n",
    "with torch.no_grad():\n",
    "    H_graph = graph_encoder(H_temporal)\n",
    "\n",
    "# ---- Assertions ----\n",
    "assert isinstance(H_graph, torch.Tensor), \"‚ùå Output is not a torch.Tensor\"\n",
    "assert H_graph.shape == (BATCH_SIZE, H), f\"‚ùå Output shape {H_graph.shape} != {(BATCH_SIZE, H)}\"\n",
    "assert H_graph.dtype == torch.float32, \"‚ùå Output dtype != float32\"\n",
    "assert torch.isfinite(H_graph).all(), \"‚ùå Non-finite values in GraphEncoder output\"\n",
    "\n",
    "# Identity check\n",
    "assert torch.allclose(H_graph, H_temporal), \"‚ùå GraphEncoder is not identity\"\n",
    "\n",
    "print(\"üìå GraphEncoder output inspection:\")\n",
    "print(f\"  Output shape: {tuple(H_graph.shape)}\")\n",
    "print(f\"  Identity mapping confirmed\")\n",
    "\n",
    "# ---- Cleanup ----\n",
    "dataset.close()\n",
    "\n",
    "print(\"\\n‚úÖ Cell 10 PASSED ‚Äî GraphEncoder is contract-compliant.\")\n",
    "print(\"‚û°Ô∏è Proceed to FusionLayer implementation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "658406bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 11: FusionLayer smoke test\n",
      "\n",
      "üìê Graph embedding shape: (4, 128)\n",
      "üìå FusionLayer output inspection:\n",
      "  Output shape: (4, 128)\n",
      "  Identity mapping confirmed\n",
      "\n",
      "‚úÖ Cell 11 PASSED ‚Äî FusionLayer is contract-compliant.\n",
      "‚û°Ô∏è Proceed to full model wiring\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 11: FusionLayer smoke test\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# ---- Ensure project root on PYTHONPATH ----\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from dataloader.temporal_dataloader import TemporalSequenceDataset\n",
    "from models.temporal_encoder import TemporalEncoder\n",
    "from models.graph_encoder import GraphEncoder\n",
    "from models.fusion import FusionLayer\n",
    "\n",
    "print(\"üß™ Running Cell 11: FusionLayer smoke test\\n\")\n",
    "\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "H5_PATH = DATASET_DIR / \"gnn_sequences_train.h5\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Load schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "F = len(feature_schema)\n",
    "H = 128\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# ---- Dataset + batch ----\n",
    "dataset = TemporalSequenceDataset(\n",
    "    h5_path=H5_PATH,\n",
    "    feature_schema_path=SCHEMA_PATH,\n",
    ")\n",
    "\n",
    "X_batch = torch.stack([dataset[i][\"X\"] for i in range(BATCH_SIZE)], dim=0)\n",
    "\n",
    "# ---- Temporal encoding ----\n",
    "temporal_encoder = TemporalEncoder(input_dim=F, hidden_dim=H)\n",
    "with torch.no_grad():\n",
    "    H_temporal = temporal_encoder(X_batch)\n",
    "\n",
    "# ---- Graph encoding ----\n",
    "graph_encoder = GraphEncoder(hidden_dim=H)\n",
    "with torch.no_grad():\n",
    "    H_graph = graph_encoder(H_temporal)\n",
    "\n",
    "print(f\"üìê Graph embedding shape: {tuple(H_graph.shape)}\")\n",
    "\n",
    "# ---- Fusion ----\n",
    "fusion = FusionLayer(hidden_dim=H)\n",
    "with torch.no_grad():\n",
    "    H_fused = fusion(H_graph)\n",
    "\n",
    "# ---- Assertions ----\n",
    "assert isinstance(H_fused, torch.Tensor), \"‚ùå Output is not a torch.Tensor\"\n",
    "assert H_fused.shape == (BATCH_SIZE, H), f\"‚ùå Output shape {H_fused.shape} != {(BATCH_SIZE, H)}\"\n",
    "assert H_fused.dtype == torch.float32, \"‚ùå Output dtype != float32\"\n",
    "assert torch.isfinite(H_fused).all(), \"‚ùå Non-finite values in FusionLayer output\"\n",
    "\n",
    "# Identity check\n",
    "assert torch.allclose(H_fused, H_graph), \"‚ùå FusionLayer is not identity\"\n",
    "\n",
    "print(\"üìå FusionLayer output inspection:\")\n",
    "print(f\"  Output shape: {tuple(H_fused.shape)}\")\n",
    "print(f\"  Identity mapping confirmed\")\n",
    "\n",
    "# ---- Cleanup ----\n",
    "dataset.close()\n",
    "\n",
    "print(\"\\n‚úÖ Cell 11 PASSED ‚Äî FusionLayer is contract-compliant.\")\n",
    "print(\"‚û°Ô∏è Proceed to full model wiring\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7420464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 12: Full model forward smoke test\n",
      "\n",
      "üìê Input batch shape: (4, 60, 263)\n",
      "üìå Model output inspection:\n",
      "  Output shape: (4, 2)\n",
      "  Sample predictions:\n",
      "tensor([[ 0.4423, -0.1023],\n",
      "        [ 0.4458, -0.2671],\n",
      "        [ 0.4327, -0.3885],\n",
      "        [ 0.4520, -0.4497]])\n",
      "\n",
      "‚úÖ Cell 12 PASSED ‚Äî Full model is contract-compliant.\n",
      "üèÅ MODEL STACK COMPLETE ‚Äî READY FOR TRAINING\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Full Model forward smoke test\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# ---- Ensure project root on PYTHONPATH ----\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from dataloader.temporal_dataloader import TemporalSequenceDataset\n",
    "from models.model import MarketSentinelModel\n",
    "\n",
    "print(\"üß™ Running Cell 12: Full model forward smoke test\\n\")\n",
    "\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "H5_PATH = DATASET_DIR / \"gnn_sequences_train.h5\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Load schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "F = len(feature_schema)\n",
    "H = 128\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# ---- Dataset + batch ----\n",
    "dataset = TemporalSequenceDataset(\n",
    "    h5_path=H5_PATH,\n",
    "    feature_schema_path=SCHEMA_PATH,\n",
    ")\n",
    "\n",
    "X_batch = torch.stack([dataset[i][\"X\"] for i in range(BATCH_SIZE)], dim=0)\n",
    "\n",
    "print(f\"üìê Input batch shape: {tuple(X_batch.shape)}\")\n",
    "\n",
    "# ---- Model ----\n",
    "model = MarketSentinelModel(\n",
    "    input_dim=F,\n",
    "    hidden_dim=H,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(X_batch)\n",
    "\n",
    "# ---- Assertions ----\n",
    "assert isinstance(y_hat, torch.Tensor), \"‚ùå Output is not a torch.Tensor\"\n",
    "assert y_hat.shape == (BATCH_SIZE, 2), f\"‚ùå Output shape {y_hat.shape} != {(BATCH_SIZE, 2)}\"\n",
    "assert y_hat.dtype == torch.float32, \"‚ùå Output dtype != float32\"\n",
    "assert torch.isfinite(y_hat).all(), \"‚ùå Non-finite values in model output\"\n",
    "\n",
    "print(\"üìå Model output inspection:\")\n",
    "print(f\"  Output shape: {tuple(y_hat.shape)}\")\n",
    "print(f\"  Sample predictions:\\n{y_hat}\")\n",
    "\n",
    "# ---- Cleanup ----\n",
    "dataset.close()\n",
    "\n",
    "print(\"\\n‚úÖ Cell 12 PASSED ‚Äî Full model is contract-compliant.\")\n",
    "print(\"üèÅ MODEL STACK COMPLETE ‚Äî READY FOR TRAINING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1701eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 13: Training dry-run test\n",
      "\n",
      "üñ• Device: cuda\n",
      "üìê X batch: (4, 60, 263) | y batch: (4, 2)\n",
      "üìâ Dry-run loss: 0.181352\n",
      "‚úÖ Backward + optimizer step completed\n",
      "\n",
      "üü¢ Cell 13 PASSED ‚Äî Training dry-run successful.\n",
      "‚û°Ô∏è You may now run training/train.py safely\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Training dry-run test (1 batch, 1 step)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---- Ensure project root on PYTHONPATH ----\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from dataloader.temporal_dataloader import TemporalSequenceDataset\n",
    "from models.model import MarketSentinelModel\n",
    "\n",
    "print(\"üß™ Running Cell 13: Training dry-run test\\n\")\n",
    "\n",
    "# ---- Device ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ• Device: {device}\")\n",
    "\n",
    "# ---- Paths ----\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "H5_PATH = DATASET_DIR / \"gnn_sequences_train.h5\"\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "\n",
    "# ---- Load schema ----\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    feature_schema = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "F = len(feature_schema)\n",
    "H = 128\n",
    "BATCH_SIZE = 4  # very safe for 4GB VRAM\n",
    "\n",
    "# ---- Dataset ----\n",
    "dataset = TemporalSequenceDataset(\n",
    "    h5_path=H5_PATH,\n",
    "    feature_schema_path=SCHEMA_PATH,\n",
    ")\n",
    "\n",
    "# ---- Build one batch ----\n",
    "X_batch = torch.stack([dataset[i][\"X\"] for i in range(BATCH_SIZE)], dim=0)\n",
    "y_batch = torch.stack([dataset[i][\"y\"] for i in range(BATCH_SIZE)], dim=0)\n",
    "\n",
    "X_batch = X_batch.to(device)\n",
    "y_batch = y_batch.to(device)\n",
    "\n",
    "print(f\"üìê X batch: {tuple(X_batch.shape)} | y batch: {tuple(y_batch.shape)}\")\n",
    "\n",
    "# ---- Model ----\n",
    "model = MarketSentinelModel(\n",
    "    input_dim=F,\n",
    "    hidden_dim=H,\n",
    ").to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# ---- Optimizer & loss ----\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# ---- Forward ----\n",
    "y_hat = model(X_batch)\n",
    "\n",
    "assert y_hat.shape == (BATCH_SIZE, 2), \"‚ùå Model output shape incorrect\"\n",
    "assert torch.isfinite(y_hat).all(), \"‚ùå Non-finite model output\"\n",
    "\n",
    "# ---- Loss ----\n",
    "loss = loss_fn(y_hat[:, 0], y_batch[:, 0]) + loss_fn(y_hat[:, 1], y_batch[:, 1])\n",
    "print(f\"üìâ Dry-run loss: {loss.item():.6f}\")\n",
    "\n",
    "# ---- Backward ----\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"‚úÖ Backward + optimizer step completed\")\n",
    "\n",
    "# ---- Cleanup ----\n",
    "dataset.close()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüü¢ Cell 13 PASSED ‚Äî Training dry-run successful.\")\n",
    "print(\"‚û°Ô∏è You may now run training/train.py safely\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c1a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 15: Checkpoint integrity check\n",
      "\n",
      "üì¶ Checkpoint file found: model_best.pt\n",
      "üìè File size: 0.58 MB\n",
      "\n",
      "‚úÖ Checkpoint loaded successfully\n",
      "üîë Checkpoint keys verified:\n",
      "  - model_state_dict\n",
      "  - epoch\n",
      "  - val_loss\n",
      "  - input_dim\n",
      "  - hidden_dim\n",
      "\n",
      "üìå Checkpoint metadata:\n",
      "  Saved epoch:     3\n",
      "  Validation loss: 0.001136\n",
      "  Input dim (F):   263\n",
      "  Hidden dim (H):  128\n",
      "\n",
      "üìä State dict sanity check:\n",
      "  Number of tensors: 6\n",
      "  Sample key:        temporal_encoder.gru.weight_ih_l0\n",
      "  Sample shape:     (384, 263)\n",
      "  Sample dtype:     torch.float32\n",
      "\n",
      "üü¢ Cell 15 PASSED ‚Äî Checkpoint is complete, valid, and loadable.\n",
      "‚û°Ô∏è The model was saved correctly. No corruption detected.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 15: Checkpoint integrity smoke test\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "print(\"üß™ Running Cell 15: Checkpoint integrity check\\n\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "CHECKPOINT_PATH = PROJECT_ROOT / \"checkpoints\" / \"model_best.pt\"\n",
    "\n",
    "# ---- Existence check ----\n",
    "assert CHECKPOINT_PATH.exists(), (\n",
    "    f\"‚ùå Checkpoint not found at {CHECKPOINT_PATH}\"\n",
    ")\n",
    "\n",
    "file_size_mb = CHECKPOINT_PATH.stat().st_size / (1024 * 1024)\n",
    "print(f\"üì¶ Checkpoint file found: {CHECKPOINT_PATH.name}\")\n",
    "print(f\"üìè File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# assert file_size_mb > 1.0, \"‚ùå Checkpoint file too small ‚Äî likely invalid\"\n",
    "\n",
    "# ---- Load checkpoint ----\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "\n",
    "print(\"\\n‚úÖ Checkpoint loaded successfully\")\n",
    "\n",
    "# ---- Key validation ----\n",
    "EXPECTED_KEYS = {\n",
    "    \"model_state_dict\",\n",
    "    \"epoch\",\n",
    "    \"val_loss\",\n",
    "    \"input_dim\",\n",
    "    \"hidden_dim\",\n",
    "}\n",
    "\n",
    "missing_keys = EXPECTED_KEYS - set(ckpt.keys())\n",
    "assert not missing_keys, f\"‚ùå Missing keys in checkpoint: {missing_keys}\"\n",
    "\n",
    "print(\"üîë Checkpoint keys verified:\")\n",
    "for k in ckpt.keys():\n",
    "    print(f\"  - {k}\")\n",
    "\n",
    "# ---- Value sanity checks ----\n",
    "assert isinstance(ckpt[\"epoch\"], int), \"‚ùå epoch is not int\"\n",
    "assert isinstance(ckpt[\"val_loss\"], float), \"‚ùå val_loss is not float\"\n",
    "assert ckpt[\"input_dim\"] > 0, \"‚ùå input_dim invalid\"\n",
    "assert ckpt[\"hidden_dim\"] > 0, \"‚ùå hidden_dim invalid\"\n",
    "\n",
    "print(\"\\nüìå Checkpoint metadata:\")\n",
    "print(f\"  Saved epoch:     {ckpt['epoch']}\")\n",
    "print(f\"  Validation loss: {ckpt['val_loss']:.6f}\")\n",
    "print(f\"  Input dim (F):   {ckpt['input_dim']}\")\n",
    "print(f\"  Hidden dim (H):  {ckpt['hidden_dim']}\")\n",
    "\n",
    "# ---- State dict sanity ----\n",
    "state_dict = ckpt[\"model_state_dict\"]\n",
    "assert isinstance(state_dict, dict), \"‚ùå model_state_dict is not a dict\"\n",
    "assert len(state_dict) > 0, \"‚ùå model_state_dict is empty\"\n",
    "\n",
    "sample_key = next(iter(state_dict))\n",
    "sample_tensor = state_dict[sample_key]\n",
    "\n",
    "assert torch.is_tensor(sample_tensor), \"‚ùå State dict values are not tensors\"\n",
    "assert torch.isfinite(sample_tensor).all(), \"‚ùå Non-finite values in checkpoint tensor\"\n",
    "\n",
    "print(\"\\nüìä State dict sanity check:\")\n",
    "print(f\"  Number of tensors: {len(state_dict)}\")\n",
    "print(f\"  Sample key:        {sample_key}\")\n",
    "print(f\"  Sample shape:     {tuple(sample_tensor.shape)}\")\n",
    "print(f\"  Sample dtype:     {sample_tensor.dtype}\")\n",
    "\n",
    "print(\"\\nüü¢ Cell 15 PASSED ‚Äî Checkpoint is complete, valid, and loadable.\")\n",
    "print(\"‚û°Ô∏è The model was saved correctly. No corruption detected.\")\n",
    "\n",
    "# # ============================================================\n",
    "# # Inspect actual contents of model_best.pt\n",
    "# # ============================================================\n",
    "\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "\n",
    "# PROJECT_ROOT = Path.cwd().parent\n",
    "# CHECKPOINT_PATH = PROJECT_ROOT / \"checkpoints\" / \"model_best.pt\"\n",
    "\n",
    "# ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "\n",
    "# print(\"Type of checkpoint object:\", type(ckpt))\n",
    "\n",
    "# if isinstance(ckpt, dict):\n",
    "#     print(\"\\nKeys found in checkpoint:\")\n",
    "#     for k in ckpt.keys():\n",
    "#         print(\"  -\", k)\n",
    "# else:\n",
    "#     print(\"\\nCheckpoint is NOT a dict.\")\n",
    "#     print(\"Likely contains a raw state_dict or model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Cell 14: Backtesting smoke test\n",
      "\n",
      "‚ö†Ô∏è  FAIR WARNING\n",
      "--------------------------------------------------\n",
      "This is a STRUCTURAL smoke test only.\n",
      "Any metrics / IC values printed here are:\n",
      "  - NOT statistically meaningful\n",
      "  - NOT strategy-evaluated\n",
      "  - NOT suitable for financial conclusions\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Required inputs present\n",
      "  - Test HDF5: gnn_sequences_test.h5\n",
      "  - Schema:   feature_schema.txt\n",
      "  - Checkpt:  model_best.pt\n",
      "\n",
      "üñ• Using device: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Checkpt:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCKPT_PATH\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ---- Execute backtest ----\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mbacktest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_h5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEST_H5\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_schema_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCHEMA_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# ---- Verify outputs ----\u001b[39;00m\n\u001b[0;32m     54\u001b[0m EXPECTED_OUTPUTS \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m     BACKTEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m     BACKTEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     57\u001b[0m     BACKTEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mic_timeseries.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     58\u001b[0m     BACKTEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal_confidence.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     59\u001b[0m ]\n",
      "File \u001b[1;32md:\\MarketSentinel\\MarketSentinel\\backtesting\\backtest.py:196\u001b[0m, in \u001b[0;36mbacktest\u001b[1;34m(test_h5, feature_schema_path, checkpoint_path)\u001b[0m\n\u001b[0;32m    193\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    197\u001b[0m         X \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    198\u001b[0m         y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\MarketSentinel\\MarketSentinel\\venv_clean\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\MarketSentinel\\MarketSentinel\\venv_clean\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\MarketSentinel\\MarketSentinel\\venv_clean\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\MarketSentinel\\MarketSentinel\\venv_clean\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\MarketSentinel\\MarketSentinel\\dataloader\\temporal_dataloader.py:118\u001b[0m, in \u001b[0;36mTemporalSequenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of bounds for dataset of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# ---- Read single sample (lazy) ----\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m X_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m          \u001b[38;5;66;03m# (60, F)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m y_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx]          \u001b[38;5;66;03m# (2,)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m sym \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbol[idx]\n",
      "File \u001b[1;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\MarketSentinel\\MarketSentinel\\venv_clean\\lib\\site-packages\\h5py\\_hl\\dataset.py:840\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, args, new_dtype)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 840\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 14: Backtesting smoke test (file integrity verification)\n",
    "# ============================================================\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def make_canary_h5(\n",
    "    src_h5: Path,\n",
    "    fraction: float,\n",
    ") -> Path:\n",
    "    assert 0 < fraction <= 1.0\n",
    "\n",
    "    tmp_dir = Path(tempfile.mkdtemp())\n",
    "    dst_h5 = tmp_dir / src_h5.name\n",
    "\n",
    "    with h5py.File(src_h5, \"r\") as src, h5py.File(dst_h5, \"w\") as dst:\n",
    "        n = src[\"X\"].shape[0]\n",
    "        k = max(1, int(n * fraction))\n",
    "\n",
    "        for key in src.keys():\n",
    "            dst.create_dataset(\n",
    "                key,\n",
    "                data=src[key][:k],\n",
    "                dtype=src[key].dtype,\n",
    "            )\n",
    "\n",
    "    return dst_h5\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# ---- Ensure project root on PYTHONPATH ----\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"üß™ Running Cell 14: Backtesting smoke test\\n\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  FAIR WARNING\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"This is a STRUCTURAL smoke test only.\")\n",
    "print(\"Any metrics / IC values printed here are:\")\n",
    "print(\"  - NOT statistically meaningful\")\n",
    "print(\"  - NOT strategy-evaluated\")\n",
    "print(\"  - NOT suitable for financial conclusions\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "from backtesting.backtest import backtest\n",
    "\n",
    "# ---- Paths ----\n",
    "DATASET_DIR = PROJECT_ROOT / \"datasets\"\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"checkpoints\"\n",
    "BACKTEST_DIR = PROJECT_ROOT / \"backtests\"\n",
    "\n",
    "# TEST_H5 = DATASET_DIR / \"gnn_sequences_test.h5\"\n",
    "FULL_TEST_H5 = DATASET_DIR / \"gnn_sequences_test.h5\"\n",
    "TEST_H5 = make_canary_h5(FULL_TEST_H5, fraction=0.05)\n",
    "\n",
    "print(f\"üß™ Canary test HDF5 created ‚Üí {TEST_H5}\")\n",
    "\n",
    "SCHEMA_PATH = DATASET_DIR / \"feature_schema.txt\"\n",
    "CKPT_PATH = CHECKPOINT_DIR / \"model_best.pt\"\n",
    "\n",
    "# ---- Sanity assertions (before execution) ----\n",
    "assert TEST_H5.exists(), \"‚ùå Test HDF5 missing\"\n",
    "assert SCHEMA_PATH.exists(), \"‚ùå feature_schema.txt missing\"\n",
    "assert CKPT_PATH.exists(), \"‚ùå model_best.pt missing\"\n",
    "\n",
    "print(\"‚úÖ Required inputs present\")\n",
    "print(f\"  - Test HDF5: {TEST_H5.name}\")\n",
    "print(f\"  - Schema:   {SCHEMA_PATH.name}\")\n",
    "print(f\"  - Checkpt:  {CKPT_PATH.name}\\n\")\n",
    "\n",
    "# ---- Execute backtest ----\n",
    "backtest(\n",
    "    test_h5=TEST_H5,\n",
    "    feature_schema_path=SCHEMA_PATH,\n",
    "    checkpoint_path=CKPT_PATH,\n",
    ")\n",
    "\n",
    "# ---- Verify outputs ----\n",
    "EXPECTED_OUTPUTS = [\n",
    "    BACKTEST_DIR / \"predictions.parquet\",\n",
    "    BACKTEST_DIR / \"metrics.json\",\n",
    "    BACKTEST_DIR / \"ic_timeseries.parquet\",\n",
    "    BACKTEST_DIR / \"signal_confidence.json\",\n",
    "]\n",
    "\n",
    "missing = [p.name for p in EXPECTED_OUTPUTS if not p.exists()]\n",
    "assert not missing, f\"‚ùå Missing backtest outputs: {missing}\"\n",
    "\n",
    "print(\"\\n‚úÖ All expected backtest artifacts created:\\n\")\n",
    "for p in EXPECTED_OUTPUTS:\n",
    "    print(f\"  - {p.name}\")\n",
    "\n",
    "# ---- Load & lightly inspect signal confidence ----\n",
    "with open(BACKTEST_DIR / \"signal_confidence.json\", \"r\") as f:\n",
    "    signal_conf = json.load(f)\n",
    "\n",
    "print(\"\\nüìå Signal confidence structure check:\")\n",
    "for horizon, stats in signal_conf.items():\n",
    "    print(f\"  {horizon}:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"    - {k}: {v}\")\n",
    "\n",
    "print(\"\\nüü¢ Cell 14 PASSED ‚Äî Backtesting pipeline is structurally sound.\")\n",
    "print(\"‚û°Ô∏è You may now trust backtesting/backtest.py as a verified artifact.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
